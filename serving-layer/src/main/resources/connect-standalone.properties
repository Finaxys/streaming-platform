# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
# Since these servers are just used for the initial connection to discover the full cluster
# membership (which may change dynamically), this list need not contain the full set of servers
# (you may want more than one, though, in case a server is down).
bootstrap.servers=localhost:9092


########################################################################


# Converter class for key Connect data. This controls the format of the data that
# will be written to Kafka for source connectors or read from Kafka for sink connectors.
# Popular formats include Avro and JSON.
key.converter=org.apache.kafka.connect.storage.StringConverter

# Converter class for value Connect data. This controls the format of the data that will
# be written to Kafka for source connectors or read from Kafka for sink connectors.
# Popular formats include Avro and JSON.
value.converter=org.apache.kafka.connect.storage.StringConverter

key.converter.schemas.enable=false
value.converter.schemas.enable=false


########################################################################


# Converter class for internal key Connect data that implements Converter interface.
# Used for converting data like offsets and configs.
internal.key.converter=org.apache.kafka.connect.storage.StringConverter

# Converter class for offset value Connect data that implements the Converter interface.
# Used for converting data like offsets and configs.
internal.value.converter=org.apache.kafka.connect.storage.StringConverter

internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false


########################################################################


# The file to store connector offsets in. By storing offsets on disk, a standalone
# process can be stopped and started on a single node and resume where it previously left off.
offset.storage.file.filename=/tmp/connect.offsets


########################################################################


# Interval at which to try committing offsets for tasks.
# Flush much faster than normal, which is useful for testing/debugging
offset.flush.interval.ms=10000

# Maximum number of milliseconds to wait for records to flush and partition
# offset data to be committed to offset storage before cancelling the process
# and restoring the offset data to be committed in a future attempt.
# Default: 5000
# offset.flush.timeout.ms


# Amount of time to wait for tasks to shutdown gracefully. This is the total amount of
# time, not per task. All task have shutdown triggered, then they are waited on sequentially.
# Default: 5000
# task.shutdown.graceful.timeout.ms


########################################################################


# If this is set, this is the hostname that will be given out to other workers to connect to.
# rest.advertised.host.name

# If this is set, this is the port that will be given out to other workers to connect to.
# rest.advertised.port

# Hostname for the REST API. If this is set, it will only bind to this interface.
# rest.host.name

# Port for the REST API to listen on.
# rest.port


